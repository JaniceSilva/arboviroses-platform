{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8f9857f",
      "metadata": {
        "id": "b8f9857f"
      },
      "source": [
        "\n",
        "# 🌐 Arboviroses — LSTM (GitHub-only, Keras `.keras`, scalers slug, push-to-GitHub)\n",
        "Notebook completo para rodar no **Google Colab**:\n",
        "- Lê **meteorologia** e **casos** direto do **GitHub** (usa `GITHUB_TOKEN` se for privado).\n",
        "- Agrega semanalmente (W-SUN), faz *merge*, cria *lags* e sequências por município.\n",
        "- Treina LSTM, avalia e **salva**: `model_lstm.keras` (nativo) + `model_lstm.h5` (compat), `metadata.json`, `scalers/` com nomes **slugificados**.\n",
        "- Gera **ZIP** com todos os artefatos e **(opcional)** envia para o GitHub via API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "667cf481",
      "metadata": {
        "id": "667cf481"
      },
      "source": [
        "## 1) Dependências (stack estável) — execute e aguarde reiniciar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104420e4",
      "metadata": {
        "id": "104420e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ Stack estável p/ Colab: TF 2.19 + Keras 3, sem conflitos\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install -U --force-reinstall --no-warn-conflicts \\\n",
        "  \"tensorflow==2.19.0\" \"keras==3.10.0\" \\\n",
        "  \"numpy==1.26.4\" \"pandas==2.2.2\" \\\n",
        "  \"scikit-learn==1.6.1\" \"joblib==1.3.2\" \\\n",
        "  \"matplotlib==3.8.4\" \"protobuf==5.29.5\" \"requests==2.32.3\"\n",
        "\n",
        "import os; os.kill(os.getpid(), 9)\n",
        "\n",
        "#!pip -q install --upgrade pip\n",
        "#!pip -q install -U --force-reinstall --no-warn-conflicts   \"tensorflow==2.19.0\" \"keras==3.10.0\"   \"numpy==1.26.4\" \"pandas==2.2.2\"   \"scikit-learn==1.6.1\" \"joblib==1.3.2\"   \"matplotlib==3.8.4\" \"protobuf==5.29.5\" \"requests==2.32.3\"\n",
        "#import os; os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, pandas, sklearn, tensorflow as tf, keras\n",
        "print(\"numpy\", numpy.__version__)\n",
        "print(\"pandas\", pandas.__version__)\n",
        "print(\"sklearn\", sklearn.__version__)\n",
        "print(\"tensorflow\", tf.__version__)\n",
        "print(\"keras\", keras.__version__)\n"
      ],
      "metadata": {
        "id": "rZolE4ivmv61"
      },
      "id": "rZolE4ivmv61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2fc1feed",
      "metadata": {
        "id": "2fc1feed"
      },
      "source": [
        "## 2) Imports e utilitários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41db1593",
      "metadata": {
        "id": "41db1593"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, io, base64, json, re, unicodedata, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt, requests\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "keras.utils.set_random_seed(SEED)\n",
        "\n",
        "ARTIFACTS_DIR = \"/content/arboviroses_artifacts\"\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "def slugify(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
        "    s = re.sub(r\"[^A-Za-z0-9_]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "print(\"OK: imports, seed e diretórios prontos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e4e1cbb",
      "metadata": {
        "id": "0e4e1cbb"
      },
      "source": [
        "## 3) Configuração do GitHub (token + caminhos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949120a1",
      "metadata": {
        "id": "949120a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
        "except Exception:\n",
        "    GITHUB_TOKEN = None\n",
        "\n",
        "OWNER   = \"JaniceSilva\"\n",
        "REPO    = \"arboviroses-platform\"\n",
        "BRANCH  = \"master\"\n",
        "\n",
        "MET_PATH_DIAM = \"backend/data/diamantina.csv\"\n",
        "MET_PATH_TEOF = \"backend/data/teofilo_otoni.csv\"\n",
        "CASES_PATH    = \"backend/data/sinan_arboviroses_data.csv\"\n",
        "\n",
        "print(\"Config GitHub OK. OWNER/REPO/BRANCH:\", OWNER, REPO, BRANCH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2352ce6",
      "metadata": {
        "id": "c2352ce6"
      },
      "source": [
        "### 3.1) Funções para ler arquivos do GitHub (API com token ou RAW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6038f951",
      "metadata": {
        "id": "6038f951"
      },
      "outputs": [],
      "source": [
        "\n",
        "def github_api_get_file(owner, repo, path, ref=\"master\", token=None):\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={ref}\"\n",
        "    headers = {\"Accept\": \"application/vnd.github+json\"}\n",
        "    if token:\n",
        "        headers[\"Authorization\"] = f\"token {token}\"\n",
        "    r = requests.get(url, headers=headers, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    if isinstance(data, dict) and \"content\" in data:\n",
        "        return base64.b64decode(data[\"content\"])\n",
        "    raise RuntimeError(f\"Resposta inesperada da API do GitHub: {str(data)[:200]}\")\n",
        "\n",
        "def github_raw_url(owner, repo, path, ref=\"master\"):\n",
        "    return f\"https://raw.githubusercontent.com/{owner}/{repo}/{ref}/{path}\"\n",
        "\n",
        "def read_csv_github(owner, repo, path, ref=\"master\", token=None, **pd_kwargs):\n",
        "    if token:\n",
        "        try:\n",
        "            content = github_api_get_file(owner, repo, path, ref, token)\n",
        "            return pd.read_csv(io.BytesIO(content), **pd_kwargs)\n",
        "        except Exception as e:\n",
        "            print(\"Aviso: falhou via API; tentando RAW…\", e)\n",
        "    url = github_raw_url(owner, repo, path, ref)\n",
        "    return pd.read_csv(url, **pd_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df150940",
      "metadata": {
        "id": "df150940"
      },
      "source": [
        "## 4) Carregar **meteorologia** do GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac6cdcf",
      "metadata": {
        "id": "cac6cdcf"
      },
      "outputs": [],
      "source": [
        "\n",
        "COLMAP_MET = {\n",
        "    \"Data Medicao\": \"date\",\n",
        "    \"PRECIPITACAO TOTAL, DIARIO (AUT)(mm)\": \"chuva\",\n",
        "    \"PRESSAO ATMOSFERICA MEDIA DIARIA (AUT)(mB)\": \"pressao_media\",\n",
        "    \"TEMPERATURA DO PONTO DE ORVALHO MEDIA DIARIA (AUT)(°C)\": \"ponto_orvalho_media\",\n",
        "    \"TEMPERATURA DO PONTO DE ORVALHO MEDIA DIARIA (AUT)(Â°C)\": \"ponto_orvalho_media\",\n",
        "    \"TEMPERATURA MAXIMA, DIARIA (AUT)(°C)\": \"temp_max\",\n",
        "    \"TEMPERATURA MAXIMA, DIARIA (AUT)(Â°C)\": \"temp_max\",\n",
        "    \"TEMPERATURA MEDIA, DIARIA (AUT)(°C)\": \"temp_media\",\n",
        "    \"TEMPERATURA MEDIA, DIARIA (AUT)(Â°C)\": \"temp_media\",\n",
        "    \"TEMPERATURA MINIMA, DIARIA (AUT)(°C)\": \"temp_min\",\n",
        "    \"TEMPERATURA MINIMA, DIARIA (AUT)(Â°C)\": \"temp_min\",\n",
        "    \"UMIDADE RELATIVA DO AR, MEDIA DIARIA (AUT)(%)\": \"umi_media\",\n",
        "    \"UMIDADE RELATIVA DO AR, MINIMA DIARIA (AUT)(%)\": \"umi_min\",\n",
        "    \"VENTO, RAJADA MAXIMA DIARIA (AUT)(m/s)\": \"vento_rajada_max\",\n",
        "    \"VENTO, VELOCIDADE MEDIA DIARIA (AUT)(m/s)\": \"vento_vel_media\",\n",
        "}\n",
        "NA_VALS = [\"\", \" \", \"-\", \"NA\", \"N/A\", None]\n",
        "\n",
        "def load_meteo_from_github(owner, repo, path, ref, token, municipio_nome):\n",
        "    df = read_csv_github(owner, repo, path, ref, token,\n",
        "                         sep=\";\", decimal=\",\", encoding=\"latin1\", engine=\"python\", na_values=NA_VALS)\n",
        "    df = df.loc[:, ~df.columns.str.match(r\"^Unnamed\")]\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    fixcols = {c: c.replace(\"(Â°C)\", \"(°C)\") for c in df.columns if \"(Â°C)\" in c}\n",
        "    if fixcols: df = df.rename(columns=fixcols)\n",
        "    ren = {c: COLMAP_MET[c] for c in df.columns if c in COLMAP_MET}\n",
        "    df = df.rename(columns=ren)\n",
        "    if \"date\" not in df.columns:\n",
        "        raise ValueError(\"Coluna de data não encontrada (esperado 'Data Medicao').\")\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", format=\"%Y-%m-%d\")\n",
        "    for v in COLMAP_MET.values():\n",
        "        if v in df.columns and v != \"date\":\n",
        "            df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
        "    df[\"municipio\"] = municipio_nome\n",
        "    return df.sort_values(\"date\").dropna(subset=[\"date\"]).reset_index(drop=True)\n",
        "\n",
        "met_diam = load_meteo_from_github(OWNER, REPO, MET_PATH_DIAM, BRANCH, (GITHUB_TOKEN or None), \"Diamantina\")\n",
        "met_teof = load_meteo_from_github(OWNER, REPO, MET_PATH_TEOF, BRANCH, (GITHUB_TOKEN or None), \"Teófilo Otoni\")\n",
        "met_all_daily = pd.concat([met_diam, met_teof], ignore_index=True)\n",
        "print(\"Meteorologia (linhas):\", met_all_daily.shape[0])\n",
        "display(met_all_daily.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d388cd",
      "metadata": {
        "id": "52d388cd"
      },
      "source": [
        "## 5) Carregar **casos** do GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3221f39c",
      "metadata": {
        "id": "3221f39c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import unicodedata\n",
        "df_cases = read_csv_github(OWNER, REPO, CASES_PATH, BRANCH, (GITHUB_TOKEN or None),\n",
        "                           encoding=\"utf-8\", sep=None, engine=\"python\")\n",
        "df_cases.columns = [c.strip().lower() for c in df_cases.columns]\n",
        "for k, v in {\"municipality\":\"municipio\",\"state\":\"uf\",\"outo date\":\"date\",\"outro date\":\"date\",\"data\":\"date\"}.items():\n",
        "    if k in df_cases.columns and v not in df_cases.columns:\n",
        "        df_cases = df_cases.rename(columns={k: v})\n",
        "if \"date\" not in df_cases.columns:\n",
        "    raise ValueError(f\"Coluna 'date' não encontrada. Colunas: {list(df_cases.columns)}\")\n",
        "df_cases[\"date\"] = pd.to_datetime(df_cases[\"date\"], errors=\"coerce\")\n",
        "for col in [\"dengue_cases\",\"zika_cases\",\"chikungunya_cases\",\"febre_amarela_cases\"]:\n",
        "    if col not in df_cases.columns:\n",
        "        df_cases[col] = 0\n",
        "    df_cases[col] = pd.to_numeric(df_cases[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "if \"total_cases\" not in df_cases.columns:\n",
        "    df_cases[\"total_cases\"] = df_cases[[\"dengue_cases\",\"zika_cases\",\"chikungunya_cases\",\"febre_amarela_cases\"]].sum(axis=1)\n",
        "def strip_accents(s):\n",
        "    if pd.isna(s): return s\n",
        "    return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(ch))\n",
        "df_cases[\"muni_norm\"] = df_cases[\"municipio\"].map(strip_accents).str.strip().str.lower()\n",
        "df_cases = df_cases[df_cases[\"muni_norm\"].isin({\"diamantina\",\"teofilo otoni\"})].drop(columns=[\"muni_norm\"]).copy()\n",
        "print(\"Casos (linhas):\", df_cases.shape[0]); display(df_cases.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc4728f",
      "metadata": {
        "id": "1bc4728f"
      },
      "source": [
        "## 6) Agregação semanal (W-SUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0bb294",
      "metadata": {
        "id": "7d0bb294"
      },
      "outputs": [],
      "source": [
        "\n",
        "agg_map = {}\n",
        "for c in met_all_daily.columns:\n",
        "    if c in [\"date\",\"municipio\"]:\n",
        "        continue\n",
        "    agg_map[c] = \"sum\" if c == \"chuva\" else \"mean\"\n",
        "\n",
        "def to_weekly(df_daily):\n",
        "    out = []\n",
        "    for muni, g in df_daily.groupby(\"municipio\", as_index=False):\n",
        "        g = g.set_index(\"date\").sort_index()\n",
        "        g_week = g.resample(\"W-SUN\").agg(agg_map)\n",
        "        g_week[\"municipio\"] = muni\n",
        "        g_week = g_week.reset_index().rename(columns={\"date\":\"date_week\"})\n",
        "        out.append(g_week)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "met_weekly = to_weekly(met_all_daily)\n",
        "casos_weekly = (\n",
        "    df_cases.set_index(\"date\").groupby(\"municipio\").resample(\"W-SUN\").agg({\n",
        "        \"dengue_cases\":\"sum\",\"zika_cases\":\"sum\",\"chikungunya_cases\":\"sum\",\"febre_amarela_cases\":\"sum\",\"total_cases\":\"sum\"\n",
        "    }).reset_index().rename(columns={\"date\":\"date_week\"})\n",
        ")\n",
        "print(\"met_weekly:\", met_weekly.shape, \"| casos_weekly:\", casos_weekly.shape)\n",
        "display(met_weekly.head(), casos_weekly.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91bf0650",
      "metadata": {
        "id": "91bf0650"
      },
      "source": [
        "## 7) Merge, features e lags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43758812",
      "metadata": {
        "id": "43758812"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_semana = pd.merge(\n",
        "    met_weekly,\n",
        "    casos_weekly[[\"date_week\",\"municipio\",\"total_cases\",\"dengue_cases\",\"zika_cases\",\"chikungunya_cases\"]],\n",
        "    on=[\"date_week\",\"municipio\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "for col in [\"dengue_cases\",\"zika_cases\",\"chikungunya_cases\"]:\n",
        "    if col not in df_semana.columns: df_semana[col] = 0\n",
        "if \"total_cases\" not in df_semana.columns:\n",
        "    df_semana[\"total_cases\"] = df_semana[[\"dengue_cases\",\"zika_cases\",\"chikungunya_cases\"]].sum(axis=1)\n",
        "df_semana[\"date_week\"] = pd.to_datetime(df_semana[\"date_week\"], errors=\"coerce\")\n",
        "for col in [\"total_cases\",\"dengue_cases\",\"zika_cases\",\"chikungunya_cases\"]:\n",
        "    df_semana[col] = pd.to_numeric(df_semana[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "df_semana = df_semana.sort_values([\"municipio\",\"date_week\"]).reset_index(drop=True)\n",
        "base_feats = [\"temp_min\",\"temp_media\",\"temp_max\",\"umi_media\",\"umi_min\",\"chuva\",\"pressao_media\",\"vento_rajada_max\",\"vento_vel_media\",\"ponto_orvalho_media\"]\n",
        "features = [c for c in base_feats if c in df_semana.columns]\n",
        "alvo = \"total_cases\"\n",
        "for lag in [1, 2]:\n",
        "    df_semana[f\"{alvo}_lag{lag}\"] = df_semana.groupby(\"municipio\")[alvo].shift(lag)\n",
        "    features.append(f\"{alvo}_lag{lag}\")\n",
        "df = df_semana.dropna(subset=features + [alvo]).copy()\n",
        "df = df.rename(columns={\"date_week\":\"date\"}).reset_index(drop=True)\n",
        "print(\"Municipios:\", df[\"municipio\"].unique().tolist())\n",
        "print(\"Período:\", df[\"date\"].min(), \"→\", df[\"date\"].max())\n",
        "print(\"N por município:\\n\", df.groupby(\"municipio\").size())\n",
        "print(\"Features:\", features); print(\"Alvo:\", alvo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bb9b7a",
      "metadata": {
        "id": "70bb9b7a"
      },
      "source": [
        "## 8) Geração de sequências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab9ab90",
      "metadata": {
        "id": "5ab9ab90"
      },
      "outputs": [],
      "source": [
        "\n",
        "WINDOW = 8; HORIZON = 1\n",
        "def make_sequences(group_df, feature_cols, target_col, window=8, horizon=1):\n",
        "    X_list, y_list, dates = [], [], []\n",
        "    X_raw = group_df[feature_cols].values; y_raw = group_df[target_col].values; dts = group_df[\"date\"].values\n",
        "    for i in range(len(group_df) - window - horizon + 1):\n",
        "        X_list.append(X_raw[i:i+window]); y_list.append(y_raw[i+window+horizon-1]); dates.append(dts[i+window+horizon-1])\n",
        "    return np.array(X_list), np.array(y_list), np.array(dates)\n",
        "X_all, y_all, dt_all, muni_all = [], [], [], []; scalers_by_muni = {}\n",
        "for muni, g in df.groupby(\"municipio\"):\n",
        "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
        "    if g.shape[0] < WINDOW + HORIZON:\n",
        "        print(f\"[PULANDO] {muni}: poucos dados.\");\n",
        "        continue\n",
        "    scaler = StandardScaler().fit(g[features].values); scalers_by_muni[muni] = scaler\n",
        "    g_scaled = g.copy(); g_scaled[features] = scaler.transform(g[features].values)\n",
        "    X, y, dts = make_sequences(g_scaled, features, alvo, window=WINDOW, horizon=HORIZON)\n",
        "    if X.size == 0:\n",
        "        print(f\"[PULANDO] {muni}: janela/horizonte grandes.\");\n",
        "        continue\n",
        "    X_all.append(X); y_all.append(y); dt_all.append(dts); muni_all.append(np.array([muni]*len(y)))\n",
        "if not X_all: raise ValueError(\"Nenhuma sequência criada.\")\n",
        "X = np.concatenate(X_all, axis=0); y = np.concatenate(y_all, axis=0)\n",
        "dates_seq = np.concatenate(dt_all, axis=0); muni_seq = np.concatenate(muni_all, axis=0)\n",
        "print(\"Shapes → X:\", X.shape, \"| y:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c6bf0b7",
      "metadata": {
        "id": "7c6bf0b7"
      },
      "source": [
        "## 9) Split treino/val/teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5f55a1",
      "metadata": {
        "id": "1c5f55a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_tmp, y_train, y_tmp, muni_train, muni_tmp = train_test_split(\n",
        "    X, y, muni_seq, test_size=0.2, random_state=SEED, shuffle=True\n",
        ")\n",
        "X_val, X_test, y_val, y_test, muni_val, muni_test = train_test_split(\n",
        "    X_tmp, y_tmp, muni_tmp, test_size=0.5, random_state=SEED, shuffle=True\n",
        ")\n",
        "X_train.shape, X_val.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f80aa5",
      "metadata": {
        "id": "b6f80aa5"
      },
      "source": [
        "## 10) Modelo LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496b70c2",
      "metadata": {
        "id": "496b70c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_timesteps = X_train.shape[1]; n_features = X_train.shape[2]\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(n_timesteps, n_features)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50534b46",
      "metadata": {
        "id": "50534b46"
      },
      "source": [
        "## 11) Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf09356",
      "metadata": {
        "id": "acf09356"
      },
      "outputs": [],
      "source": [
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\")]\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=64, callbacks=callbacks, verbose=1)\n",
        "plt.figure(); plt.plot(hist.history[\"loss\"], label=\"train\"); plt.plot(hist.history[\"val_loss\"], label=\"val\"); plt.legend(); plt.title(\"Curva de treino (MSE)\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d6aa18",
      "metadata": {
        "id": "57d6aa18"
      },
      "source": [
        "## 12) Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2545aa4",
      "metadata": {
        "id": "e2545aa4"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Teste — MSE: {test_loss:.3f} | MAE: {test_mae:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa3c96d",
      "metadata": {
        "id": "9aa3c96d"
      },
      "source": [
        "## 13) Salvar artefatos (.keras + .h5), metadata, scalers (slug), ZIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a00f58f",
      "metadata": {
        "id": "4a00f58f"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_path_keras = os.path.join(ARTIFACTS_DIR, \"model_lstm.keras\")\n",
        "model_path_h5    = os.path.join(ARTIFACTS_DIR, \"model_lstm.h5\")\n",
        "savedmodel_dir   = os.path.join(ARTIFACTS_DIR, \"model_lstm_savedmodel\")\n",
        "ok_keras = False\n",
        "try:\n",
        "    model.save(model_path_keras); ok_keras = True; print(\"✅ .keras salvo:\", model_path_keras)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ .keras falhou:\", e); print(\"➡️ usando SavedModel…\"); model.save(savedmodel_dir); print(\"✅ SavedModel salvo:\", savedmodel_dir)\n",
        "try:\n",
        "    model.save(model_path_h5); print(\"✅ .h5 salvo:\", model_path_h5)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ .h5 falhou:\", e)\n",
        "metadata = {\n",
        "    \"window\": int(WINDOW), \"horizon\": int(HORIZON), \"features\": list(features), \"target\": \"total_cases\",\n",
        "    \"framework\": \"keras\", \"tensorflow_version\": keras.__version__,\n",
        "    \"artifacts\": {\"keras\": model_path_keras if ok_keras else None, \"h5\": model_path_h5, \"savedmodel_dir\": savedmodel_dir if not ok_keras else None}\n",
        "}\n",
        "with open(os.path.join(ARTIFACTS_DIR, \"metadata.json\"), \"w\") as f: json.dump(metadata, f, indent=2, default=str)\n",
        "scalers_dir = os.path.join(ARTIFACTS_DIR, \"scalers\"); os.makedirs(scalers_dir, exist_ok=True)\n",
        "for muni, g in df.groupby(\"municipio\"):\n",
        "    sc = StandardScaler().fit(g[features].values); out_name = f\"scaler_{slugify(muni)}.joblib\"; joblib.dump(sc, os.path.join(scalers_dir, out_name))\n",
        "from google.colab import files\n",
        "!cd {ARTIFACTS_DIR} && zip -r artifacts_bundle.zip * -q\n",
        "print(\"Conteúdo de artifacts:\", os.listdir(ARTIFACTS_DIR))\n",
        "files.download(os.path.join(ARTIFACTS_DIR, \"artifacts_bundle.zip\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17bea8cb",
      "metadata": {
        "id": "17bea8cb"
      },
      "source": [
        "## 14) Função de inferência por município (scaler slugificado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805435df",
      "metadata": {
        "id": "805435df"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_next_for_muni(model, df_all, muni, last_window_df, features, artifacts_dir=ARTIFACTS_DIR, window=8):\n",
        "    import joblib, numpy as np\n",
        "    path_s = os.path.join(artifacts_dir, \"scalers\", f\"scaler_{slugify(muni)}.joblib\")\n",
        "    if not os.path.exists(path_s): raise FileNotFoundError(f\"Scaler não encontrado: {path_s}\")\n",
        "    scaler_m = joblib.load(path_s)\n",
        "    X_last = last_window_df.sort_values(\"date\").copy()\n",
        "    X_scaled = scaler_m.transform(X_last[features].values)\n",
        "    X_seq = np.expand_dims(X_scaled, axis=0)\n",
        "    return float(model.predict(X_seq, verbose=0).ravel()[0])\n",
        "\n",
        "for muni in df[\"municipio\"].unique().tolist():\n",
        "    g = df[df[\"municipio\"] == muni].sort_values(\"date\")\n",
        "    if g.shape[0] >= WINDOW:\n",
        "        print(muni, \"→ previsão próxima semana:\", round(predict_next_for_muni(model, df, muni, g.tail(WINDOW), features, window=WINDOW), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84244d19",
      "metadata": {
        "id": "84244d19"
      },
      "source": [
        "## 15) (Opcional) Enviar artefatos para o GitHub (backend/artifacts/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2229ccc",
      "metadata": {
        "id": "b2229ccc"
      },
      "outputs": [],
      "source": [
        "\n",
        "if GITHUB_TOKEN:\n",
        "    import base64, requests\n",
        "    DEST_DIR = \"backend/artifacts\"\n",
        "    def github_upsert(owner, repo, path, content_bytes, message, branch=\"master\", token=None):\n",
        "        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
        "        headers = {\"Accept\":\"application/vnd.github+json\"}\n",
        "        if token: headers[\"Authorization\"] = f\"token {token}\"\n",
        "        # pegar SHA se já existe\n",
        "        sha = None\n",
        "        r0 = requests.get(url+f\"?ref={branch}\", headers=headers, timeout=60)\n",
        "        if r0.status_code == 200:\n",
        "            try: sha = r0.json().get(\"sha\")\n",
        "            except Exception: sha = None\n",
        "        data = {\"message\": message, \"content\": base64.b64encode(content_bytes).decode(\"utf-8\"), \"branch\": branch}\n",
        "        if sha: data[\"sha\"] = sha\n",
        "        r = requests.put(url, headers=headers, data=json.dumps(data), timeout=60); r.raise_for_status(); return r.json()\n",
        "    # principais\n",
        "    for fname in [\"model_lstm.keras\", \"model_lstm.h5\", \"metadata.json\"]:\n",
        "        fpath = os.path.join(ARTIFACTS_DIR, fname)\n",
        "        if os.path.exists(fpath):\n",
        "            with open(fpath, \"rb\") as f:\n",
        "                github_upsert(OWNER, REPO, f\"{DEST_DIR}/{fname}\", f.read(), message=f\"Add {fname} (LSTM artifacts)\", branch=BRANCH, token=GITHUB_TOKEN)\n",
        "            print(\"Enviado:\", fname)\n",
        "    # scalers\n",
        "    scdir = os.path.join(ARTIFACTS_DIR, \"scalers\")\n",
        "    for fname in sorted(os.listdir(scdir)):\n",
        "        if fname.endswith(\".joblib\"):\n",
        "            fpath = os.path.join(scdir, fname)\n",
        "            with open(fpath, \"rb\") as f:\n",
        "                github_upsert(OWNER, REPO, f\"{DEST_DIR}/scalers/{fname}\", f.read(), message=f\"Add scaler {fname}\", branch=BRANCH, token=GITHUB_TOKEN)\n",
        "            print(\"Enviado scaler:\", fname)\n",
        "else:\n",
        "    print(\"GITHUB_TOKEN não definido; pulando push para GitHub.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}